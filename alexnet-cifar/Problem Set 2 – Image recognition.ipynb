{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we have briefly reviewed the idea of learning good features directly from data and went through the concept of convolutional neural networks along with few architectures.\n",
    "\n",
    "Until recently, building convolutional neural networks was tough. There was no high-level tools for that, you would be required to understand all the internal mechanics of the model and its operations.\n",
    "\n",
    "Today, due to the high-level tools such as Keras and TensorFlow, everybody can build a convolutional neural network and put it to work without diving deep into them. What used to be a one-month project became a few hours exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.backend import resize_images\n",
    "\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (cv_images, cv_labels) = cifar10.load_data()\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np_utils.to_categorical(np.array(train_labels), 10)\n",
    "\n",
    "cv_images = np.array(cv_images)\n",
    "cv_labels = np_utils.to_categorical(np.array(cv_labels), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(cv_images.shape)\n",
    "print(len(cv_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(np_array):\n",
    "    %matplotlib inline\n",
    "    plt.figure()\n",
    "    plt.imshow(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(data_set, labels, example_index):\n",
    "    show_image(data_set[example_index])\n",
    "    print('Label: ', labels[example_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(train_images, train_labels, example_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(cv_images, cv_labels, example_index = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2012 a convolutional neural network called AlexNet won ImageNet competition. \n",
    "\n",
    "Go through an [original AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) to investigate the architecture. Next, investigate the [basics of Keras](https://keras.io/#keras-the-python-deep-learning-library). We will use it with TensorFlow backend.\n",
    "\n",
    "You are also encouraged to go through some CNN tutorial for Keras. There is a number of them online (for example, [this](https://elitedatascience.com/keras-tutorial-deep-learning-in-python) or [this](https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/)).\n",
    "Now, build AlexNex network with Keras for object recognition. Note that standard AlexNet works with 224x224 input images. The dataset you are going to use for this problem is 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_17 (Lambda)           (None, 54, 54, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 16, 16, 48)        3648      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 8, 8, 64)          76864     \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 8, 8, 96)          55392     \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 8, 8, 96)          83040     \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 8, 8, 64)          55360     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 605,066\n",
      "Trainable params: 605,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Layer 0\n",
    "# resize images to have the same input as in the AlexNet\n",
    "model.add(Lambda(lambda image: resize_images(image, height_factor=7, width_factor=7, data_format='channels_last'), input_shape=(32,32,3), output_shape=(54,54,3)))\n",
    "\n",
    "# AlexNet layers description here: https://sushscience.wordpress.com/2016/12/04/understanding-alexnet/\n",
    "# Keras layers docs: https://keras.io/layers/core/\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(filters=48, kernel_size=(11,11), strides=4, activation='relu', padding='same'))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu', padding='same'))\n",
    "\n",
    "# Layer 3\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters=192, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv2D(filters=192, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "\n",
    "# Layer 6\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048, activation = 'relu'))\n",
    "\n",
    "# Layer 7\n",
    "model.add(Dense(2048, activation = 'relu'))\n",
    "\n",
    "# Layer 8\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use training set for training the network to recognize objects. You might want to use RMSProp optimizer to speed up the training.\n",
    "\n",
    "Convolutional networks require a lot of computing power for training. Typical setup for training CNN is to use GPU, however, in this problem you are not required to do so. CPU will be fine as well.\n",
    "\n",
    "If you are using CPU for this subproblem, training process might be slow. You can stop it manually as soon as you get meaningful results.\n",
    "\n",
    "Report the results on the training and cross-validation sets. The report should contain the training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 29s 574us/step - loss: 1.6539 - acc: 0.4049 - val_loss: 1.4542 - val_acc: 0.4832\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 28s 555us/step - loss: 1.2915 - acc: 0.5400 - val_loss: 1.2476 - val_acc: 0.5457\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 28s 556us/step - loss: 1.1636 - acc: 0.5878 - val_loss: 1.2349 - val_acc: 0.5659\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 27s 550us/step - loss: 1.0616 - acc: 0.6276 - val_loss: 1.1818 - val_acc: 0.5844\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 28s 562us/step - loss: 0.9888 - acc: 0.6513 - val_loss: 1.0597 - val_acc: 0.6342\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 28s 560us/step - loss: 0.9344 - acc: 0.6705 - val_loss: 1.0959 - val_acc: 0.6247\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 28s 554us/step - loss: 0.8877 - acc: 0.6906 - val_loss: 1.1022 - val_acc: 0.6201\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 28s 556us/step - loss: 0.8464 - acc: 0.7032 - val_loss: 1.0804 - val_acc: 0.6386\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 28s 560us/step - loss: 0.8044 - acc: 0.7160 - val_loss: 1.0441 - val_acc: 0.6381\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 28s 567us/step - loss: 0.7663 - acc: 0.7293 - val_loss: 1.0931 - val_acc: 0.6428\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 28s 552us/step - loss: 0.7404 - acc: 0.7393 - val_loss: 1.0626 - val_acc: 0.6490\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 28s 565us/step - loss: 0.7005 - acc: 0.7539 - val_loss: 1.1432 - val_acc: 0.6357\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 28s 567us/step - loss: 0.6824 - acc: 0.7608 - val_loss: 1.1566 - val_acc: 0.6484\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 28s 566us/step - loss: 0.6486 - acc: 0.7741 - val_loss: 1.1170 - val_acc: 0.6519\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 28s 565us/step - loss: 0.6259 - acc: 0.7816 - val_loss: 1.1725 - val_acc: 0.6452\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 28s 558us/step - loss: 0.6066 - acc: 0.7882 - val_loss: 1.2011 - val_acc: 0.6500\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 28s 562us/step - loss: 0.5682 - acc: 0.8013 - val_loss: 1.2023 - val_acc: 0.6502\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 27s 548us/step - loss: 0.5496 - acc: 0.8122 - val_loss: 1.2755 - val_acc: 0.6495\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 28s 553us/step - loss: 0.5286 - acc: 0.8164 - val_loss: 1.2527 - val_acc: 0.6423\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 28s 552us/step - loss: 0.5048 - acc: 0.8247 - val_loss: 1.2371 - val_acc: 0.6473\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 28s 554us/step - loss: 0.4948 - acc: 0.8305 - val_loss: 1.3148 - val_acc: 0.6418\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 28s 553us/step - loss: 0.4727 - acc: 0.8366 - val_loss: 1.4079 - val_acc: 0.6481\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 28s 552us/step - loss: 0.4630 - acc: 0.8427 - val_loss: 1.4274 - val_acc: 0.6283\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 28s 558us/step - loss: 0.4312 - acc: 0.8534 - val_loss: 1.5318 - val_acc: 0.6453\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 28s 553us/step - loss: 0.4334 - acc: 0.8527 - val_loss: 1.3728 - val_acc: 0.6382\n",
      "Epoch 26/100\n",
      "14400/50000 [=======>......................] - ETA: 18s - loss: 0.3612 - acc: 0.8779"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-8c328b522973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/plaidml/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplaidml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/plaidml/keras/backend.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(value, dtype, name, constraint)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         _device(), plaidml.Shape(_ctx, ptile.convert_np_dtype_to_pml(dtype), *value.shape))\n\u001b[1;32m   1668\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmap_discard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_from_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     return ptile.Value.from_var(tensor, value.shape, ptile.convert_np_dtype_to_pml(dtype),\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/plaidml/__init__.py\u001b[0m in \u001b[0;36mcopy_from_ndarray\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m   1171\u001b[0m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uint16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypeslib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/ctypeslib.py\u001b[0m in \u001b[0;36mas_array\u001b[0;34m(obj, shape)\u001b[0m\n\u001b[1;32m    515\u001b[0m                     'pointer')\n\u001b[1;32m    516\u001b[0m             \u001b[0mp_arr_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOINTER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ctype_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_arr_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, batch_size=64, epochs=100, initial_epoch=0, verbose=1, validation_data=(cv_images, cv_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load models: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_name + \".h5\")\n",
    "    print('Saved', model_name, 'to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The best accuracy on the test set is 0.6167 (epoch 33)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, AlexNet does not work very well on such a small dataset. Recall what you have learned from this class to improve its performance. You can also take a look at the [Dropout technique](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) and its [implementation in Keras](https://keras.io/layers/core/#dropout). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_34 (Lambda)           (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64, 64, 3)         12        \n",
      "_________________________________________________________________\n",
      "conv2d_95 (Conv2D)           (None, 16, 16, 48)        3648      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 8, 8, 64)          76864     \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 8, 8, 96)          55392     \n",
      "_________________________________________________________________\n",
      "conv2d_98 (Conv2D)           (None, 8, 8, 96)          83040     \n",
      "_________________________________________________________________\n",
      "conv2d_99 (Conv2D)           (None, 8, 8, 64)          55360     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 605,078\n",
      "Trainable params: 605,072\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dropout_model = Sequential()\n",
    "\n",
    "# Layer 0\n",
    "# resize images to have the same input as in the AlexNet\n",
    "dropout_model.add(Lambda(lambda image: resize_images(image, height_factor=2, width_factor=2, data_format='channels_last'), input_shape=(32,32,3), output_shape=(64,64,3)))\n",
    "\n",
    "dropout_model.add(BatchNormalization())\n",
    "\n",
    "# AlexNet layers description here: https://sushscience.wordpress.com/2016/12/04/understanding-alexnet/\n",
    "# Keras layers docs: https://keras.io/layers/core/\n",
    "\n",
    "# Layer 1\n",
    "dropout_model.add(Conv2D(filters=48, kernel_size=(5,5), strides=4, activation='relu', padding='same'))\n",
    "\n",
    "# Layer 2\n",
    "dropout_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "dropout_model.add(Conv2D(filters=64, kernel_size=(5,5), activation='relu', padding='same'))\n",
    "\n",
    "# Layer 3\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "dropout_model.add(Conv2D(filters=96, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "\n",
    "# Layer 4\n",
    "dropout_model.add(Conv2D(filters=96, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "\n",
    "# Layer 5\n",
    "dropout_model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "\n",
    "# Layer 6\n",
    "dropout_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "dropout_model.add(Flatten())\n",
    "dropout_model.add(Dense(256, activation = 'relu'))\n",
    "# dropout_model.add(Dropout(0.2))\n",
    "\n",
    "# Layer 7\n",
    "dropout_model.add(Dense(256, activation = 'relu'))\n",
    "dropout_model.add(Dropout(0.20))\n",
    "\n",
    "# Layer 8\n",
    "dropout_model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "print(dropout_model.summary())\n",
    "\n",
    "dropout_model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 133 of 488 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.6072 - acc: 0.4080"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 139 of 488 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 48s 961us/step - loss: 1.6069 - acc: 0.4081 - val_loss: 1.3866 - val_acc: 0.5025\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 29s 586us/step - loss: 1.2119 - acc: 0.5656 - val_loss: 1.1621 - val_acc: 0.5872\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 29s 586us/step - loss: 1.0272 - acc: 0.6367 - val_loss: 1.0120 - val_acc: 0.6447\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 29s 585us/step - loss: 0.9014 - acc: 0.6841 - val_loss: 0.8994 - val_acc: 0.6856\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 29s 586us/step - loss: 0.7992 - acc: 0.7199 - val_loss: 0.8509 - val_acc: 0.6966\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 29s 587us/step - loss: 0.7174 - acc: 0.7509 - val_loss: 0.8282 - val_acc: 0.7159\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 29s 584us/step - loss: 0.6456 - acc: 0.7743 - val_loss: 0.8225 - val_acc: 0.7169\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 29s 587us/step - loss: 0.5714 - acc: 0.8003 - val_loss: 0.8044 - val_acc: 0.7336\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 29s 586us/step - loss: 0.5075 - acc: 0.8229 - val_loss: 0.8294 - val_acc: 0.7269\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 29s 588us/step - loss: 0.4469 - acc: 0.8434 - val_loss: 0.8089 - val_acc: 0.7312\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 29s 589us/step - loss: 0.3874 - acc: 0.8641 - val_loss: 0.8573 - val_acc: 0.7278\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 29s 588us/step - loss: 0.3352 - acc: 0.8809 - val_loss: 0.9808 - val_acc: 0.7195\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 29s 587us/step - loss: 0.2903 - acc: 0.8969 - val_loss: 0.9871 - val_acc: 0.7234\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 29s 587us/step - loss: 0.2500 - acc: 0.9126 - val_loss: 1.1605 - val_acc: 0.7189\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 29s 589us/step - loss: 0.2343 - acc: 0.9174 - val_loss: 1.0791 - val_acc: 0.7220\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 29s 588us/step - loss: 0.1989 - acc: 0.9297 - val_loss: 1.1316 - val_acc: 0.7265\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 29s 588us/step - loss: 0.1822 - acc: 0.9357 - val_loss: 1.2311 - val_acc: 0.7221\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 29s 587us/step - loss: 0.1613 - acc: 0.9433 - val_loss: 1.2564 - val_acc: 0.7213\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 29s 587us/step - loss: 0.1467 - acc: 0.9487 - val_loss: 1.3078 - val_acc: 0.7240\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 29s 587us/step - loss: 0.1404 - acc: 0.9521 - val_loss: 1.3714 - val_acc: 0.7178\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 29s 587us/step - loss: 0.1194 - acc: 0.9585 - val_loss: 1.4665 - val_acc: 0.7228\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 29s 588us/step - loss: 0.1254 - acc: 0.9558 - val_loss: 1.5389 - val_acc: 0.7139\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 29s 589us/step - loss: 0.1061 - acc: 0.9626 - val_loss: 1.4551 - val_acc: 0.7189\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 29s 588us/step - loss: 0.1099 - acc: 0.9636 - val_loss: 1.6178 - val_acc: 0.7205\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 29s 586us/step - loss: 0.1027 - acc: 0.9653 - val_loss: 1.6109 - val_acc: 0.7165\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 29s 587us/step - loss: 0.1035 - acc: 0.9647 - val_loss: 1.5363 - val_acc: 0.7256\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 29s 589us/step - loss: 0.0905 - acc: 0.9692 - val_loss: 1.4443 - val_acc: 0.7246\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 29s 587us/step - loss: 0.1047 - acc: 0.9648 - val_loss: 1.5696 - val_acc: 0.7223\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 29s 585us/step - loss: 0.0792 - acc: 0.9733 - val_loss: 1.7217 - val_acc: 0.7209\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 29s 586us/step - loss: 0.0906 - acc: 0.9696 - val_loss: 1.6967 - val_acc: 0.7128\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 29s 585us/step - loss: 0.0897 - acc: 0.9697 - val_loss: 1.6134 - val_acc: 0.7234\n",
      "Epoch 32/50\n",
      "29696/50000 [================>.............] - ETA: 11s - loss: 0.0601 - acc: 0.9801"
     ]
    }
   ],
   "source": [
    "dropout_model.fit(train_images, train_labels, batch_size=256, epochs=50, initial_epoch=0, verbose=1, validation_data=(cv_images, cv_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(dropout_model, 'dropout_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The best accuracy on the test set is 0.6199 (epoch 27)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
